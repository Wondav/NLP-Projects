{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34070ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     --------------------------------------- 81.4/81.4 kB 79.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: multiprocess in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: pandas in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (2022.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: dill in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (10.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from pandas->evaluate) (2022.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wonder-david efe\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cb33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ba8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('model/')\n",
    "model = TFAutoModelForTokenClassification.from_pretrained('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3991b0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 2.4652464389801025,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 6338,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fb58851be7455eb40559706994fe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:02<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be2f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"I am a boy but i behave like a girl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015123b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(inputs, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0af70384",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Exception encountered when calling layer \"tf_bert_for_token_classification\" \"                 f\"(type TFBertForTokenClassification).\n\nlist index out of range\n\nCall arguments received by layer \"tf_bert_for_token_classification\" \"                 f\"(type TFBertForTokenClassification):\n  • input_ids=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • attention_mask=['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n  • token_type_ids=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:419\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m--> 419\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:511\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         output[parameter_names[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, allowed_types) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m     output[\u001b[43mparameter_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparameter_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n",
      "\u001b[1;31mIndexError\u001b[0m: Exception encountered when calling layer \"tf_bert_for_token_classification\" \"                 f\"(type TFBertForTokenClassification).\n\nlist index out of range\n\nCall arguments received by layer \"tf_bert_for_token_classification\" \"                 f\"(type TFBertForTokenClassification):\n  • input_ids=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • attention_mask=['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n  • token_type_ids=['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "model(**input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9456c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
